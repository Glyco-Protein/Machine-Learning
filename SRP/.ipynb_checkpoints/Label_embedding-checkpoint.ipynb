{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c41b6709",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertConfig,BertTokenizer,BertModel,BertForMultipleChoice\n",
    "from torch import nn\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "from d2l import torch as d2l\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4a8ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据\n",
    "excel_file = 'D:/基于深度学习的海量文本处理/第1阶段/10w.xlsx'\n",
    "data_frame = pd.read_excel(excel_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b3f54b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词预处理\n",
    "stop_words = ['您好','你好','很高兴为您服务','请问有什么可以帮您','client','user',':',' ']\n",
    "def ProcessStopWords(text):\n",
    "    for word in stop_words:\n",
    "        text = text.replace(word,'')\n",
    "    return text\n",
    "\n",
    "data_frame['转写文本'] = data_frame['转写文本'].map(ProcessStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ca7cf86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签预处理\n",
    "def ProcessLabels(text):\n",
    "    word = '>>'\n",
    "    text = text.replace(word,'>')\n",
    "    return text\n",
    "\n",
    "data_frame['服务请求'] = data_frame['服务请求'].map(ProcessLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afa56f2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    哎我想问一下,我这个一三九的号码那个扣费方式是怎么样的扣费方式的话它是每个月呢就是每个月的一...\n",
       "1    喂我那个宽带不能用宽带故障了是吗哦现在不能用前前几天就不能用了稍等我帮您看一下嗯稍等一下3天...\n",
       "2    哎我我办理这个那个我想问一下我这个号码有没有开通5g套餐呀我看一下您这里有开通5g业务可以享...\n",
       "3    请不要挂机您拨叫的用户正在通话中请唔好挂机您拨叫凯用户正在通话中嗯由于您多次没有声音我将结束...\n",
       "4    喂我想咨询一下我的那个联通卡怎么回事啊,我都没用过嗯您可以提供一下吗这个号码就是我我号码我都...\n",
       "5    嗯我想问一下我这个卡是啊升了5g上次是帮我升了5g然后现在是月租是54.5元上网费又是30一...\n",
       "6                                                不要我有卡\n",
       "7    我的号码为啥暂暂停服务了稍等一下我帮您看了一下的话您之前的话有反映过这个问题的是吧然后的话我...\n",
       "8    哎你查一下我这个话费还有余额吗您这边还有8.95元怎么打不了怎么没有信号呢网络没有了呢呃信号...\n",
       "9    哎先生哦嗯我想问一下现在如果这个号码要补号码的话那个身份证复印件有没有有效的补卡补卡要原件的...\n",
       "Name: 转写文本, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 预览预处理结果\n",
    "data_frame['转写文本'].iloc[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "caf8c270",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = np.array(data_frame['转写文本'])\n",
    "choices = np.array(data_frame['服务请求'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "790333c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((526,), (100000, 1))"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 去重choices，并保存原choices对应去重后的位置\n",
    "unique_choices = np.unique(choices)\n",
    "labels = np.array([np.argwhere(unique_choices==v)[0] for v in choices])\n",
    "unique_choices.shape, labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cab75b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_features = tokenizer(features.tolist(),return_tensors='pt',padding=True, truncation=True,max_length=512)\n",
    "# all_labels = tokenizer(labels.tolist(),return_tensors='pt',padding=True, truncation=True,max_length=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "64a5c3ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基于 BertForMultipleChoice 模型\n",
    "class BertForMutipleChoiceModel(nn.Module):\n",
    "    def __init__(self, UseGPU, model_path, batch_size):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.model_path = model_path\n",
    "        self.model_config = BertConfig.from_pretrained(self.model_path)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.model_path)\n",
    "        self.bertchoice = BertForMultipleChoice.from_pretrained(self.model_path, config=self.model_config)\n",
    "        self.try_gpu(UseGPU)\n",
    "        \n",
    "    def try_gpu(self, bUse):\n",
    "        if torch.cuda.device_count() and bUse == True >= 1:\n",
    "            self.device = 'cuda:0'\n",
    "        else:\n",
    "            self.device = 'cpu'\n",
    "        self.bertchoice.to(self.device) # 迁移模型到device\n",
    "    \n",
    "    def format_choices(self, choices_text):\n",
    "        '''choices_text是已去重的np 1d array，且对应的答案label在外部维护。对答案进行全局编码，方便后续使用，后续训练只需传入labels号'''\n",
    "#         self.choices = tokenizer(choices_text,return_tensors='pt',padding=True, truncation=True,max_length=100)\n",
    "        self.num_choices = choices_text.shape[0] # 答案的数量\n",
    "        self.choices = np.repeat(choices_text, self.batch_size, axis=0).reshape((self.batch_size,-1)).tolist()\n",
    "        \n",
    "    def forward(self, prompt, labels):\n",
    "        '''prompt是未编码的问题文本 1d np array（batch_size）, label是对应这个问题的答案标号，第0维都为batch_size'''\n",
    "        '''labels 为 (batch_size, 1)'''\n",
    "        prompts =  np.reshape(np.repeat(prompt, self.num_choices, axis=1),(self.num_choices, -1)) # 重复问题准备拼接\n",
    "        encoding = self.tokenizer(prompts, self.choices, return_tensors='pt', padding=True)\n",
    "        outputs = self.bertchoice(**{k: v for k, v in encoding.items()}, labels=labels) # 第 0 维为 batch_size\n",
    "        # the linear classifier still needs to be trained\n",
    "#         loss = outputs.loss\n",
    "#         logits = outputs.logits\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "f9270184",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at ./bert-base-chinese/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['哈哈哈', '哈哈哈'], ['你好啊', '你好啊']]\n"
     ]
    }
   ],
   "source": [
    "# 测试代码\n",
    "m = BertForMutipleChoiceModel(False, './bert-base-chinese/', 2)\n",
    "choices  = np.array(['哈哈哈','你好啊'])\n",
    "m.format_choices(choices)\n",
    "print(m.choices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e850f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建 Dataloader 准备训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b85ec66f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForMultipleChoice were not initialized from the model checkpoint at ./bert-base-chinese/ and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids\n",
      "tensor([[  101,   100,   100,   117, 10315,  9342,  8180, 11667,  8217, 12244,\n",
      "          8315,  9738, 13288,   117, 11541,  8370,  8975,  8243,   143, 10637,\n",
      "           117,  8310, 11685, 11836,  8303,   163,  8727, 11809,  8303,   119,\n",
      "           102,   100,  8310,  9714, 11598,  8663,   143,  8330,  8197,  8256,\n",
      "           143,   153,  8833,  9568,   119,   102],\n",
      "        [  101,   100,   100,   117, 10315,  9342,  8180, 11667,  8217, 12244,\n",
      "          8315,  9738, 13288,   117, 11541,  8370,  8975,  8243,   143, 10637,\n",
      "           117,  8310, 11685, 11836,  8303,   163,  8727, 11809,  8303,   119,\n",
      "           102,   100,  8310,  9714, 11598,   165,  8963,  8268,  9245,  8635,\n",
      "          8217,  8174, 12126,  8168,   119,   102]])\n",
      "token_type_ids\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "attention_mask\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "# 官方示例代码\n",
    "m = BertForMultipleChoice.from_pretrained(model_path)\n",
    "prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n",
    "choice0 = \"It is eaten with a fork and a knife.\"\n",
    "choice1 = \"It is eaten while held in the hand.\"\n",
    "labels = torch.tensor(0).unsqueeze(0)  # choice0 is correct (according to Wikipedia ;)), batch size 1\n",
    "\n",
    "# tokenizer 会将pair对应位置拼接并用[SEP]分割\n",
    "encoding = tokenizer([prompt, prompt], [choice0, choice1], return_tensors=\"pt\", padding=True)\n",
    "outputs = m(**{k: v.unsqueeze(0) for k, v in encoding.items()}, labels=labels)  # batch size is 1\n",
    "\n",
    "# the linear classifier still needs to be trained\n",
    "loss = outputs.loss\n",
    "logits = outputs.logits\n",
    "for k,v in encoding.items():\n",
    "    print(k)\n",
    "    print(v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
